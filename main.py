# ê´€ë ¨ ë§í¬: https://www.kaggle.com/datasets/atomicd/retail-store-inventory-and-demand-forecasting

# ============================================
# ì„í¬íŠ¸ & ì „ì—­ë³€ìˆ˜
# ============================================
import pandas as pd
import numpy as np
import os

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
import plot

import optimizer
from path_helper import PROJECT_DIR

USE_GPU = True
RANDOM_SEED = 100
OPTIMIZER_TIME_OUT = 1200
CATBOOST_MODEL_PTH = PROJECT_DIR / "model" / "catboost_model.cbm"
XGBOOST_MODEL_PTH = PROJECT_DIR / "model" / "xgb_model.json"
LIGHTGBM_MODEL_PTH = PROJECT_DIR / "model" / "lightgbm_model.txt"
# ============================================
# 1. ë°ì´í„° ì „ì²˜ë¦¬
# ============================================

# -----------
# ë°ì´í„° ë¡œë“œ ë° ì»¬ëŸ¼ ì²˜ë¦¬
# -----------
df = pd.read_csv(PROJECT_DIR / 'dataset' / 'sales_data.csv')


# -----------
# ê²°ì¸¡ì¹˜ íƒì§€
# -----------
print("========= ê²°ì¸¡ì¹˜ ì²´í¬ =========")
print(df.isna().sum().sort_values(ascending=False))

# -----------
# Date íŒŒìƒ ë³€ìˆ˜ ìƒì„±
# -----------
df["Date"] = pd.to_datetime(df["Date"])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day_of_Week'] = df['Date'].dt.dayofweek

df.drop(labels=["Date"], axis=1, inplace=True)

# -----------
# ìˆœì„œí˜• ì¸ì½”ë”©
# -----------
order = {"Spring": 0, "Summer": 1, "Autumn": 2, "Winter": 3}
df["Seasonality"] = df["Seasonality"].map(order)

# -----------
# ë¼ë²¨ ì¸ì½”ë”© (ì¹´í…Œê³ ë¦¬)
# -----------
cat_cols = ["Region", "Category", "Weather Condition", "Product ID", "Store ID"]
for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    df[col] = df[col].astype("category")

# -----------
# ìˆ˜ì¹˜í˜• ë°ì´í„°ë“¤ í‘œì¤€í™”
# -----------
scar_cols = ['Price', 'Units Ordered', 'Units Sold', 'Inventory Level', 'Competitor Pricing']
scaler = StandardScaler()
df[scar_cols] = scaler.fit_transform(df[scar_cols])

print(df.head())

# ============================================
# 2. ëª¨ë¸ í•™ìŠµì‹œí‚¤ê³  testì…‹ì„ ì´ìš©í•´ì„œ ì˜ˆì¸¡ê²°ê³¼(y_pred) ë½‘ê¸°
# ============================================

# -------------------
# Train/Testì…‹ ë§Œë“¤ê¸°
# -------------------
X = df.drop(columns='Demand')
y = df["Demand"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)

# -------------------
# XGBoost ì‹¤í–‰
# -------------------
print("========= XGBoost ì‹¤í–‰ =========")

import xgboost as xgb
xgb_model = None

if os.path.exists(XGBOOST_MODEL_PTH):
    print(f"ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {XGBOOST_MODEL_PTH}")
    xgb_model = xgb.XGBRegressor()
    xgb_model.load_model(XGBOOST_MODEL_PTH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
else:
    print("ğŸš€ ìƒˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    print("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    study = optimizer.optimize_xgb(X_train=X_train, use_cuda=USE_GPU, seed=RANDOM_SEED, y_train=y_train, timeout=OPTIMIZER_TIME_OUT)

    print("ğŸ”§ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
    params = study.best_params.copy()
    params.update(
        n_estimators=300,
        enable_categorical=True,
        tree_method="hist",
        device="cuda" if USE_GPU else "cpu",
        objective="reg:squarederror",
        eval_metric="rmse",
        random_state=RANDOM_SEED,
    )
    xgb_model = xgb.XGBRegressor(**params)
    xgb_model.fit(X_train, y_train)
    print("ğŸ”§ í•™ìŠµì™„ë£Œ! ëª¨ë¸íŒŒì¼ì„ ì €ì¥ì¤‘ì…ë‹ˆë‹¤...")

    os.makedirs(os.path.dirname(XGBOOST_MODEL_PTH), exist_ok=True)
    xgb_model.save_model(XGBOOST_MODEL_PTH)
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

dtest = xgb.DMatrix(X_test, enable_categorical=True)
xgb_y_pred = xgb_model.get_booster().predict(dtest, validate_features=False)


# -------------------
# LightGBM ì‹¤í–‰
# -------------------
print("========= LightGBM ì‹¤í–‰ =========")

import lightgbm
lgbm_model = None

if os.path.exists(LIGHTGBM_MODEL_PTH):
    print(f"ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {LIGHTGBM_MODEL_PTH}")
    lgbm_model = lightgbm.Booster(model_file=LIGHTGBM_MODEL_PTH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
else:
    print("ğŸš€ ìƒˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    print("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    study = optimizer.optimize_lgbm(X_train=X_train, y_train=y_train, use_gpu=USE_GPU, categorical_feature=cat_cols, timeout=OPTIMIZER_TIME_OUT)
    params = study.best_params.copy()
    params.update({
        "verbose": 100,
        "random_state": RANDOM_SEED,
        "boosting_type": "gbdt",
        "n_jobs": -1,
        "device": "gpu" if USE_GPU else "cpu",
        "force_row_wise": True,
        "max_depth": -1,
    })
    params.update(n_estimators=300)

    print("ğŸ”§ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
    lgbm_model = lightgbm.LGBMRegressor(**params)
    lgbm_model.fit(X_train, y_train, categorical_feature=cat_cols)

    print("ğŸ”§ í•™ìŠµì™„ë£Œ! ëª¨ë¸íŒŒì¼ì„ ì €ì¥ì¤‘ì…ë‹ˆë‹¤...")
    os.makedirs(os.path.dirname(LIGHTGBM_MODEL_PTH), exist_ok=True)
    lgbm_model = lgbm_model.booster_
    lgbm_model.save_model(LIGHTGBM_MODEL_PTH)
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

lgbm_y_pred = lgbm_model.predict(X_test)

# -------------------
# CatBoost ì‹¤í–‰
# -------------------
print("========= CatBoost ì‹¤í–‰ =========")
import catboost 

cat_model = None

if os.path.exists(CATBOOST_MODEL_PTH):
    print(f"ğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë°œê²¬: {CATBOOST_MODEL_PTH}")
    cat_model = catboost.CatBoostRegressor()
    cat_model.load_model(CATBOOST_MODEL_PTH)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
else:
    print("ğŸš€ ìƒˆ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

    print("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    study = optimizer.optimize_catboost(
        X_train=X_train, y_train=y_train,
        cat_features=cat_cols,
        timeout=OPTIMIZER_TIME_OUT,
        use_gpu=USE_GPU,
        seed=RANDOM_SEED
    )
    params = study.best_params.copy()
    params.update({
        "random_seed": RANDOM_SEED,
        "allow_writing_files": False,
        "bootstrap_type": "Bernoulli",
        "verbose": 100,
    })
    if USE_GPU:
        params.update({"task_type": "GPU", "devices": "0"})
    params.update(iterations=300)

    print("ğŸ”§ ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
    cat_model = catboost.CatBoostRegressor(**params)
    cat_model.fit(X_train, y_train, cat_features=cat_cols)

    print("ğŸ”§ í•™ìŠµì™„ë£Œ! ëª¨ë¸íŒŒì¼ì„ ì €ì¥ì¤‘ì…ë‹ˆë‹¤...")
    os.makedirs(os.path.dirname(CATBOOST_MODEL_PTH), exist_ok=True)
    cat_model.save_model(CATBOOST_MODEL_PTH)
    print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

cat_y_pred = cat_model.predict(X_test)

# ============================================
# 3. ì‹œê°í™” í•˜ê¸°
# ============================================

# -----------------------------------------------
# ê° ëª¨ë¸ì˜ MAE, MSE, RMSE, MAPE ë„ì¶œí•˜ê³  ì‹œê°í™”í•˜ê¸°
# ----------------------------------------------
def conv_pred2dict(y_pred, y_test, name):
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_test, y_pred)
    return {
        'Model': name,
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': f'{mape * 100:.2f}%'
    }

pred_list = [("CatBoost", cat_y_pred), ("XGBooster", xgb_y_pred), ("LightGBM", lgbm_y_pred)]
results = []
for name, pred in pred_list:
    results.append(conv_pred2dict(pred, y_test, name))

df_result = pd.DataFrame(results)
plot.performance_table(df_result)

# ----------------------------------------------
# ê° ëª¨ë¸ì˜ ì£¼ìš”ë³€ìˆ˜ ì¶œë ¥í•˜ê¸°
# ----------------------------------------------
cat_importances = cat_model.get_feature_importance(data=catboost.Pool(X_train, label=y_train, cat_features=cat_cols))
importances = [
    ("CatBoost", cat_importances),
    ("XGBoost", xgb_model.feature_importances_),
    ("LightGBM", lgbm_model.feature_importance()),
]
plot.feature_importance(importances, X_train.columns)